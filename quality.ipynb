{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from helper.loader import load_ratings, load_movies, load_lists, load_500_1000_corr\n",
    "from helper.preprocesser import preprocess_ratings, prepare_test_data_dense\n",
    "from helper.application import application\n",
    "import helper.collaborative as coll\n",
    "import helper.showResults as sh\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "Notebook to check the performance of our different models by predicting ratings which we manually removed to have a ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = load_ratings()\n",
    "movies = load_movies()\n",
    "lists = load_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_new, lists_new = preprocess_ratings(ratings, lists, 500,1000)\n",
    "dense_user_item = coll.get_dense_user_item(ratings_new)\n",
    "popu_matrix = coll.get_popularity(lists_new, dense_user_item)\n",
    "\n",
    "average_ratings = coll.compute_average_ratings(dense_user_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3188952"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dense_user_item.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starting with only 1/1000 of data missing, this represent 3188 ratings to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8890.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label = prepare_test_data_dense(dense_user_item,1/300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense_user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key1,key2 = list(train_label.keys())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense_user_item.columns[key2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Each removed ratings is for a new user -> will have to compute a lot of prediction, maybe something cleaner but didn't see\n",
    "\n",
    "classic_norm_SSE = 0\n",
    "classic_SSE = 0\n",
    "hybrid_norm_SSE = 0\n",
    "hybrid_SSE = 0\n",
    "pop_norm_SSE = 0\n",
    "pop_SSE = 0\n",
    "\n",
    "rounded_classic_norm_error = 0\n",
    "rounded_classic_error = 0\n",
    "rounded_hybrid_norm_error = 0\n",
    "rounded_hybrid_error = 0\n",
    "rounded_pop_norm_error = 0\n",
    "rounded_pop_error = 0\n",
    "\n",
    "\n",
    "for key1,key2 in tqdm(train_label.keys()):\n",
    "\n",
    "    user = train_data.index[key1]\n",
    "    item = train_data.columns[key2]\n",
    "\n",
    "    similarity = coll.get_k_dynamic_similar_users(train_data,user,40)\n",
    "    popularity = coll.get_k_popularity(popu_matrix,similarity)\n",
    "\n",
    "    ##################\n",
    "    weight_classic = similarity\n",
    "    \n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_classic,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_classic,train_data,average_ratings)\n",
    "\n",
    "    classic_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    classic_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_classic_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_classic_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    hybrid = similarity.loc[user] + popularity.loc[\"popularity\"]\n",
    "    weight_hybrid = hybrid.to_frame().transpose() \n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_hybrid,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_hybrid,train_data,average_ratings)\n",
    "\n",
    "    hybrid_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    hybrid_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_hybrid_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_hybrid_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    weight_pop = popularity\n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_pop,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_pop,train_data,average_ratings)\n",
    "        \n",
    "    pop_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    pop_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_pop_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_pop_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "    ##################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For classical model: \n",
      "With normalization: average SSE = 0.20546733649878557, average rounded diff = 0.3\n",
      "Without normalization: average SSE = 0.18302398249376867, average rounded diff = 0.2\n",
      "For Hybrid model: \n",
      "With normalization: average SSE = 0.18888464924174314, average rounded diff = 0.3\n",
      "Without normalization: average SSE = 0.17094301720658348, average rounded diff = 0.2\n",
      "For Popularity only based model: \n",
      "With normalization: average SSE = 0.12329947094744798, average rounded diff = 0.2\n",
      "Without normalization: average SSE = 0.34396025560666804, average rounded diff = 0.4\n"
     ]
    }
   ],
   "source": [
    "nbr = len(train_label)\n",
    "\n",
    "print(\"For classical model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(classic_norm_SSE/nbr,rounded_classic_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(classic_SSE/nbr,rounded_classic_error/nbr))\n",
    "\n",
    "print(\"For Hybrid model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_norm_SSE/nbr,rounded_hybrid_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_SSE/nbr,rounded_hybrid_error/nbr))\n",
    "\n",
    "print(\"For Popularity only based model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(pop_norm_SSE/nbr,rounded_pop_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(pop_SSE/nbr,rounded_pop_error/nbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 10900.69it/s]\n",
      "100%|██████████| 31/31 [00:24<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For classical model: \n",
      "With normalization: average SSE = 0.8581998990867361, average rounded diff = 0.7419354838709677\n",
      "Without normalization: average SSE = 1.1175007472736047, average rounded diff = 0.7741935483870968\n",
      "For Hybrid model: \n",
      "With normalization: average SSE = 0.8915128001507262, average rounded diff = 0.7741935483870968\n",
      "Without normalization: average SSE = 1.1264048736707912, average rounded diff = 0.8064516129032258\n",
      "For Popularity only based model: \n",
      "With normalization: average SSE = 1.0320785799561667, average rounded diff = 0.7419354838709677\n",
      "Without normalization: average SSE = 1.242506356928144, average rounded diff = 0.9032258064516129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label = prepare_test_data_dense(dense_user_item,1/100000)\n",
    "\n",
    "####################\n",
    "classic_norm_SSE = 0\n",
    "classic_SSE = 0\n",
    "hybrid_norm_SSE = 0\n",
    "hybrid_SSE = 0\n",
    "pop_norm_SSE = 0\n",
    "pop_SSE = 0\n",
    "\n",
    "rounded_classic_norm_error = 0\n",
    "rounded_classic_error = 0\n",
    "rounded_hybrid_norm_error = 0\n",
    "rounded_hybrid_error = 0\n",
    "rounded_pop_norm_error = 0\n",
    "rounded_pop_error = 0\n",
    "\n",
    "\n",
    "for key1,key2 in tqdm(train_label.keys()):\n",
    "\n",
    "    user = train_data.index[key1]\n",
    "    item = train_data.columns[key2]\n",
    "\n",
    "    similarity = coll.get_k_dynamic_similar_users(train_data,user,40)\n",
    "    popularity = coll.get_k_popularity(popu_matrix,similarity)\n",
    "\n",
    "    ##################\n",
    "    weight_classic = similarity\n",
    "    \n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_classic,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_classic,train_data,average_ratings)\n",
    "\n",
    "    classic_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    classic_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_classic_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_classic_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    hybrid = similarity.loc[user] + popularity.loc[\"popularity\"]\n",
    "    weight_hybrid = hybrid.to_frame().transpose() \n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_hybrid,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_hybrid,train_data,average_ratings)\n",
    "\n",
    "    hybrid_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    hybrid_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_hybrid_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_hybrid_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    weight_pop = popularity\n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_pop,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_pop,train_data,average_ratings)\n",
    "        \n",
    "    pop_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    pop_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_pop_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_pop_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "    ##################\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "nbr = len(train_label)\n",
    "\n",
    "print(\"For classical model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(classic_norm_SSE/nbr,rounded_classic_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(classic_SSE/nbr,rounded_classic_error/nbr))\n",
    "\n",
    "print(\"For Hybrid model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_norm_SSE/nbr,rounded_hybrid_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_SSE/nbr,rounded_hybrid_error/nbr))\n",
    "\n",
    "print(\"For Popularity only based model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(pop_norm_SSE/nbr,rounded_pop_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(pop_SSE/nbr,rounded_pop_error/nbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 300 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [00:00<00:00, 12290.83it/s]\n",
      "100%|██████████| 318/318 [03:46<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For classical model: \n",
      "With normalization: average SSE = 0.7112991706439104, average rounded diff = 0.6540880503144654\n",
      "Without normalization: average SSE = 0.7927014635308044, average rounded diff = 0.6509433962264151\n",
      "For Hybrid model: \n",
      "With normalization: average SSE = 0.7178691268824478, average rounded diff = 0.6477987421383647\n",
      "Without normalization: average SSE = 0.804285097995739, average rounded diff = 0.6792452830188679\n",
      "For Popularity only based model: \n",
      "With normalization: average SSE = 0.939116679942665, average rounded diff = 0.7327044025157232\n",
      "Without normalization: average SSE = 1.0880041268901652, average rounded diff = 0.7830188679245284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label = prepare_test_data_dense(dense_user_item,1/10000)\n",
    "\n",
    "####################\n",
    "classic_norm_SSE = 0\n",
    "classic_SSE = 0\n",
    "hybrid_norm_SSE = 0\n",
    "hybrid_SSE = 0\n",
    "pop_norm_SSE = 0\n",
    "pop_SSE = 0\n",
    "\n",
    "rounded_classic_norm_error = 0\n",
    "rounded_classic_error = 0\n",
    "rounded_hybrid_norm_error = 0\n",
    "rounded_hybrid_error = 0\n",
    "rounded_pop_norm_error = 0\n",
    "rounded_pop_error = 0\n",
    "\n",
    "\n",
    "for key1,key2 in tqdm(train_label.keys()):\n",
    "\n",
    "    user = train_data.index[key1]\n",
    "    item = train_data.columns[key2]\n",
    "\n",
    "    similarity = coll.get_k_dynamic_similar_users(train_data,user,40)\n",
    "    popularity = coll.get_k_popularity(popu_matrix,similarity)\n",
    "\n",
    "    ##################\n",
    "    weight_classic = similarity\n",
    "    \n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_classic,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_classic,train_data,average_ratings)\n",
    "\n",
    "    classic_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    classic_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_classic_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_classic_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    hybrid = similarity.loc[user] + popularity.loc[\"popularity\"]\n",
    "    weight_hybrid = hybrid.to_frame().transpose() \n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_hybrid,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_hybrid,train_data,average_ratings)\n",
    "\n",
    "    hybrid_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    hybrid_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_hybrid_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_hybrid_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    weight_pop = popularity\n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_pop,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_pop,train_data,average_ratings)\n",
    "        \n",
    "    pop_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    pop_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_pop_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_pop_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "    ##################\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "nbr = len(train_label)\n",
    "\n",
    "print(\"For classical model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(classic_norm_SSE/nbr,rounded_classic_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(classic_SSE/nbr,rounded_classic_error/nbr))\n",
    "\n",
    "print(\"For Hybrid model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_norm_SSE/nbr,rounded_hybrid_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_SSE/nbr,rounded_hybrid_error/nbr))\n",
    "\n",
    "print(\"For Popularity only based model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(pop_norm_SSE/nbr,rounded_pop_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(pop_SSE/nbr,rounded_pop_error/nbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3'000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3188/3188 [00:00<00:00, 12980.09it/s]\n",
      "100%|██████████| 3188/3188 [37:11<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For classical model: \n",
      "With normalization: average SSE = 0.7031799656614639, average rounded diff = 0.6031994981179423\n",
      "Without normalization: average SSE = 0.7890107127694462, average rounded diff = 0.6558971141781681\n",
      "For Hybrid model: \n",
      "With normalization: average SSE = 0.7138551456235214, average rounded diff = 0.6126097867001254\n",
      "Without normalization: average SSE = 0.7997318753692487, average rounded diff = 0.6577791718946048\n",
      "For Popularity only based model: \n",
      "With normalization: average SSE = 0.909063123812653, average rounded diff = 0.6985570890840652\n",
      "Without normalization: average SSE = 1.042783065090846, average rounded diff = 0.7728983688833124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label = prepare_test_data_dense(dense_user_item,1/1000)\n",
    "\n",
    "####################\n",
    "classic_norm_SSE = 0\n",
    "classic_SSE = 0\n",
    "hybrid_norm_SSE = 0\n",
    "hybrid_SSE = 0\n",
    "pop_norm_SSE = 0\n",
    "pop_SSE = 0\n",
    "\n",
    "rounded_classic_norm_error = 0\n",
    "rounded_classic_error = 0\n",
    "rounded_hybrid_norm_error = 0\n",
    "rounded_hybrid_error = 0\n",
    "rounded_pop_norm_error = 0\n",
    "rounded_pop_error = 0\n",
    "\n",
    "\n",
    "for key1,key2 in tqdm(train_label.keys()):\n",
    "\n",
    "    user = train_data.index[key1]\n",
    "    item = train_data.columns[key2]\n",
    "\n",
    "    similarity = coll.get_k_dynamic_similar_users(train_data,user,40)\n",
    "    popularity = coll.get_k_popularity(popu_matrix,similarity)\n",
    "\n",
    "    ##################\n",
    "    weight_classic = similarity\n",
    "    \n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_classic,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_classic,train_data,average_ratings)\n",
    "\n",
    "    classic_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    classic_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_classic_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_classic_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    hybrid = similarity.loc[user] + popularity.loc[\"popularity\"]\n",
    "    weight_hybrid = hybrid.to_frame().transpose() \n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_hybrid,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_hybrid,train_data,average_ratings)\n",
    "\n",
    "    hybrid_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    hybrid_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_hybrid_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_hybrid_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    weight_pop = popularity\n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_pop,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_pop,train_data,average_ratings)\n",
    "        \n",
    "    pop_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    pop_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_pop_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_pop_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "    ##################\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "nbr = len(train_label)\n",
    "\n",
    "print(\"For classical model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(classic_norm_SSE/nbr,rounded_classic_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(classic_SSE/nbr,rounded_classic_error/nbr))\n",
    "\n",
    "print(\"For Hybrid model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_norm_SSE/nbr,rounded_hybrid_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_SSE/nbr,rounded_hybrid_error/nbr))\n",
    "\n",
    "print(\"For Popularity only based model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(pop_norm_SSE/nbr,rounded_pop_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(pop_SSE/nbr,rounded_pop_error/nbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10'000 data missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10629/10629 [00:00<00:00, 12554.26it/s]\n",
      "100%|██████████| 10629/10629 [2:17:17<00:00,  1.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For classical model: \n",
      "With normalization: average SSE = 0.6844750574019053, average rounded diff = 0.5932825289302851\n",
      "Without normalization: average SSE = 0.7804481943849714, average rounded diff = 0.6519898391193903\n",
      "For Hybrid model: \n",
      "With normalization: average SSE = 0.6924351646501989, average rounded diff = 0.5982688870072443\n",
      "Without normalization: average SSE = 0.7894809729666076, average rounded diff = 0.6556590460062094\n",
      "For Popularity only based model: \n",
      "With normalization: average SSE = 0.8721110264495707, average rounded diff = 0.6853890300122307\n",
      "Without normalization: average SSE = 1.0139553582783285, average rounded diff = 0.7556684542289961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data,train_label = prepare_test_data_dense(dense_user_item,1/300)\n",
    "\n",
    "####################\n",
    "classic_norm_SSE = 0\n",
    "classic_SSE = 0\n",
    "hybrid_norm_SSE = 0\n",
    "hybrid_SSE = 0\n",
    "pop_norm_SSE = 0\n",
    "pop_SSE = 0\n",
    "\n",
    "rounded_classic_norm_error = 0\n",
    "rounded_classic_error = 0\n",
    "rounded_hybrid_norm_error = 0\n",
    "rounded_hybrid_error = 0\n",
    "rounded_pop_norm_error = 0\n",
    "rounded_pop_error = 0\n",
    "\n",
    "\n",
    "for key1,key2 in tqdm(train_label.keys()):\n",
    "\n",
    "    user = train_data.index[key1]\n",
    "    item = train_data.columns[key2]\n",
    "\n",
    "    similarity = coll.get_k_dynamic_similar_users(train_data,user,40)\n",
    "    popularity = coll.get_k_popularity(popu_matrix,similarity)\n",
    "\n",
    "    ##################\n",
    "    weight_classic = similarity\n",
    "    \n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_classic,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_classic,train_data,average_ratings)\n",
    "\n",
    "    classic_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    classic_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_classic_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_classic_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    hybrid = similarity.loc[user] + popularity.loc[\"popularity\"]\n",
    "    weight_hybrid = hybrid.to_frame().transpose() \n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_hybrid,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_hybrid,train_data,average_ratings)\n",
    "\n",
    "    hybrid_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    hybrid_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_hybrid_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_hybrid_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "\n",
    "    ##################\n",
    "    weight_pop = popularity\n",
    "\n",
    "    pred_norm = coll.predict_value_norm(user,item,weight_pop,train_data,average_ratings)\n",
    "    pred = coll.predict_value(user,item,weight_pop,train_data,average_ratings)\n",
    "        \n",
    "    pop_norm_SSE += (pred_norm - train_label[(key1,key2)])**2\n",
    "    pop_SSE += (pred - train_label[(key1,key2)])**2\n",
    "\n",
    "    rounded_pop_norm_error += abs(round(pred_norm) - train_label[(key1,key2)])\n",
    "    rounded_pop_error += abs(round(pred) - train_label[(key1,key2)])\n",
    "    ##################\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "nbr = len(train_label)\n",
    "\n",
    "print(\"For classical model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(classic_norm_SSE/nbr,rounded_classic_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(classic_SSE/nbr,rounded_classic_error/nbr))\n",
    "\n",
    "print(\"For Hybrid model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_norm_SSE/nbr,rounded_hybrid_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(hybrid_SSE/nbr,rounded_hybrid_error/nbr))\n",
    "\n",
    "print(\"For Popularity only based model: \")\n",
    "print(\"With normalization: average SSE = {}, average rounded diff = {}\".format(pop_norm_SSE/nbr,rounded_pop_norm_error/nbr))\n",
    "print(\"Without normalization: average SSE = {}, average rounded diff = {}\".format(pop_SSE/nbr,rounded_pop_error/nbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting everything for 10 removed ratings just to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9414.82it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_2,train_label_2 = prepare_test_data_dense(dense_user_item,1/300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For user 70929901 and movie 2083, the true ratings was 4.0\n",
      "The classical model predicted 4.366124772998244 with normalization and 4.08230020935102 without\n",
      "The hybrid model predicted 4.334833886793348 with normalization and 4.036882293536396 without\n",
      "The popularity model predicted 3.4741589719285715 with normalization and 2.7876348107032083 without\n",
      "################################\n",
      "For user 10040458 and movie 402, the true ratings was 4.0\n",
      "The classical model predicted 5 with normalization and 4.237505533225593 without\n",
      "The hybrid model predicted 4.889349607306551 with normalization and 4.234722687670517 without\n",
      "The popularity model predicted 4.579815126019368 with normalization and 4.227625664078463 without\n",
      "################################\n",
      "For user 33246005 and movie 4124, the true ratings was 3.0\n",
      "The classical model predicted 2.2888367633349596 with normalization and 2.584491556826118 without\n",
      "The hybrid model predicted 2.2888367633349596 with normalization and 2.584491556826118 without\n",
      "The popularity model predicted 3.210059171597633 with normalization and 3.210059171597633 without\n",
      "################################\n",
      "For user 72658609 and movie 1898, the true ratings was 4.0\n",
      "The classical model predicted 3.744142624784343 with normalization and 3.334468488942111 without\n",
      "The hybrid model predicted 3.5913152156644474 with normalization and 3.166705781528962 without\n",
      "The popularity model predicted 3.0229566498614595 with normalization and 2.5428034833112343 without\n",
      "################################\n",
      "For user 57608812 and movie 484, the true ratings was 4.0\n",
      "The classical model predicted 3.6230829745108615 with normalization and 4.429650213531347 without\n",
      "The hybrid model predicted 3.6229883697950767 with normalization and 4.392579387545019 without\n",
      "The popularity model predicted 3.622674836031536 with normalization and 4.269721288002398 without\n",
      "################################\n",
      "For user 66919492 and movie 18458, the true ratings was 2.0\n",
      "The classical model predicted 3.630698512831744 with normalization and 3.089474286642055 without\n",
      "The hybrid model predicted 3.6187455358587 with normalization and 3.068835692687787 without\n",
      "The popularity model predicted 3.4740938194984583 with normalization and 2.8190729728666373 without\n",
      "################################\n",
      "For user 96698926 and movie 3369, the true ratings was 1.0\n",
      "The classical model predicted 2.242869953684625 with normalization and 2.150068554800779 without\n",
      "The hybrid model predicted 2.1764017201052344 with normalization and 2.078081817836586 without\n",
      "The popularity model predicted 1.7154856163363794 with normalization and 1.5788983050847458 without\n",
      "################################\n",
      "For user 55327630 and movie 2092, the true ratings was 5.0\n",
      "The classical model predicted 5 with normalization and 4.71743110766274 without\n",
      "The hybrid model predicted 5 with normalization and 4.721366316362284 without\n",
      "The popularity model predicted 5 with normalization and 5.0 without\n",
      "################################\n",
      "For user 15720652 and movie 1299, the true ratings was 5.0\n",
      "The classical model predicted 5 with normalization and 4.497475426280832 without\n",
      "The hybrid model predicted 5 with normalization and 4.386588285109926 without\n",
      "The popularity model predicted 4.964112679955703 with normalization and 4.0 without\n",
      "################################\n",
      "For user 5627368 and movie 399, the true ratings was 5.0\n",
      "The classical model predicted 4.132060478672064 with normalization and 3.993119980463581 without\n",
      "The hybrid model predicted 3.9420767092704345 with normalization and 3.7804696497912276 without\n",
      "The popularity model predicted 3.470977469294541 with normalization and 3.2531645569620253 without\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "for key1,key2 in train_label_2.keys():\n",
    "\n",
    "    user = train_data_2.index[key1]\n",
    "    item = train_data_2.columns[key2]\n",
    "\n",
    "    similarity = coll.get_k_dynamic_similar_users(train_data_2,user,40)\n",
    "    popularity = coll.get_k_popularity(popu_matrix,similarity)\n",
    "\n",
    "    ##################\n",
    "    weight_classic = similarity\n",
    "    \n",
    "    cla_pred_norm = coll.predict_value_norm(user,item,weight_classic,train_data_2,average_ratings)\n",
    "    cla_pred = coll.predict_value(user,item,weight_classic,train_data_2,average_ratings)\n",
    "\n",
    "    ##################\n",
    "    hybrid = similarity.loc[user] + popularity.loc[\"popularity\"]\n",
    "    weight_hybrid = hybrid.to_frame().transpose() \n",
    "\n",
    "    hyb_pred_norm = coll.predict_value_norm(user,item,weight_hybrid,train_data_2,average_ratings)\n",
    "    hyb_pred = coll.predict_value(user,item,weight_hybrid,train_data_2,average_ratings)\n",
    "\n",
    "    ##################\n",
    "    weight_pop = popularity\n",
    "\n",
    "    pop_pred_norm = coll.predict_value_norm(user,item,weight_pop,train_data_2,average_ratings)\n",
    "    pop_pred = coll.predict_value(user,item,weight_pop,train_data_2,average_ratings)\n",
    "    ##################\n",
    "\n",
    "    print(\"For user {} and movie {}, the true ratings was {}\".format(user,item,train_label_2[(key1,key2)]))\n",
    "    print(\"The classical model predicted {} with normalization and {} without\".format(cla_pred_norm,cla_pred))\n",
    "    print(\"The hybrid model predicted {} with normalization and {} without\".format(hyb_pred_norm,hyb_pred))\n",
    "    print(\"The popularity model predicted {} with normalization and {} without\".format(pop_pred_norm,pop_pred))\n",
    "    print(\"################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "903e7e2862ff4215d6e17c2a7262d9cc840f26402d44e286ed2eb13102c7459e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('social')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
